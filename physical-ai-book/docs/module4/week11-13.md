---
id: week11-13
title: "Weeks 11-13: Humanoid Development & VLA"
sidebar_label: "Weeks 11-13: Humanoid & VLA"
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Weeks 11-13: Humanoid Development & VLA

<Tabs>
  <TabItem value="full" label="Full Content" default>

## Part 1: Humanoid Robot Development (Weeks 11-12)

Building a humanoid is the ultimate engineering challenge. It requires mastering the physics of instability.

### The Inverted Pendulum Problem
A walking human is essentially an inverted pendulum that is constantly falling and catching itself.
*   **ZMP (Zero Moment Point):** The traditional control theory approach. We calculate the point where the total inertial force is zero and ensure it stays within the support polygon (the feet).
*   **Reinforcement Learning (RL):** The modern approach. We let the robot "play" in simulation (Isaac Lab) millions of times until it learns to walk.

> **Case Study: Unitree G1**
> The Unitree G1 uses a hybrid approach. It has high-torque motors (up to 120Nm) and runs a policy trained in Isaac Gym using **PPO (Proximal Policy Optimization)**. This allows it to recover from pushes and walk on uneven terrain.

### Kinematics: Forward vs. Inverse
*   **Forward Kinematics (FK):** "If I set my joint angles to X, where is my hand?" (Easy).
*   **Inverse Kinematics (IK):** "I want my hand at position Y, what should my joint angles be?" (Hard).
    *   For humanoids, IK is computationally expensive because they have high Degrees of Freedom (DoF). We use solvers like **Pinocchio** or **KDL** within ROS 2.

---

## Part 2: The Cognitive Brain (Week 13)

Now that our robot can walk, let's give it a mind. We will build a **VLA (Vision-Language-Action)** pipeline.

### Step 1: Hearing (OpenAI Whisper)
We need to convert audio waves into text.

**Python Code: ROS 2 Whisper Node**
```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import whisper
import speech_recognition as sr

class WhisperNode(Node):
    def __init__(self):
        super().__init__('whisper_node')
        self.publisher_ = self.create_publisher(String, '/human/voice_command', 10)
        self.model = whisper.load_model("base")
        self.recognizer = sr.Recognizer()
        self.timer = self.create_timer(0.1, self.listen_and_transcribe)

    def listen_and_transcribe(self):
        with sr.Microphone() as source:
            # Listen for audio (blocking for simplicity)
            audio = self.recognizer.listen(source)
            try:
                # Save to temp file and transcribe
                with open("temp.wav", "wb") as f:
                    f.write(audio.get_wav_data())
                result = self.model.transcribe("temp.wav")
                text = result["text"]
                
                msg = String()
                msg.data = text
                self.publisher_.publish(msg)
                self.get_logger().info(f'Heard: "{text}"')
            except Exception as e:
                self.get_logger().error(f'Error: {e}')

def main():
    rclpy.init()
    node = WhisperNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()
```

### Step 2: Thinking (LLM Planning)
We feed the transcribed text into an LLM (like GPT-4o) along with a "System Prompt" that defines the robot's capabilities.

**System Prompt Example:**
```text
You are a robot assistant. You can perform the following actions:
1. navigate_to(location)
2. pick_up(object)
3. place(object, location)

User: "Put the apple in the trash."
Plan:
1. navigate_to("table")
2. pick_up("apple")
3. navigate_to("trash_can")
4. place("apple", "trash_can")
```

### Step 3: Acting (ROS 2 Action Client)
The LLM outputs a plan (e.g., JSON or a list). We parse this plan and execute it using ROS 2 Action Clients.

**Python Code: Action Client**
```python
from rclpy.action import ActionClient
from nav2_msgs.action import NavigateToPose

class RobotBrain(Node):
    def __init__(self):
        super().__init__('robot_brain')
        self._action_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')

    def send_goal(self, x, y):
        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.pose.position.x = x
        goal_msg.pose.pose.position.y = y
        
        self._action_client.wait_for_server()
        return self._action_client.send_goal_async(goal_msg)
```

## The Capstone: The Autonomous Humanoid
Combine everything!
1.  **Voice Command:** "Go to the kitchen."
2.  **Whisper:** Transcribes audio.
3.  **LLM:** Decides to call `navigate_to("kitchen")`.
4.  **Nav2:** Plans a path avoiding obstacles.
5.  **Locomotion:** The RL policy moves the legs to follow the path.

  </TabItem>
  <TabItem value="summary" label="Summary">

## Summary: Humanoid & VLA

**Goal:** A robot that walks like a human and thinks like an AI.

### Key Technologies
*   **Locomotion:** RL (PPO) is replacing ZMP for robust walking.
*   **Whisper:** Real-time, offline-capable speech recognition.
*   **LLMs:** The "Prefrontal Cortex" of the robot, enabling high-level planning.
*   **VLA Models:** The futureâ€”end-to-end models that go directly from pixels/text to motor torques (e.g., Google RT-2).

### The "Sim-to-Real" Challenge
Training these cognitive models requires massive data. We use **Isaac Sim** to generate synthetic "experiences" for the VLA models before deploying them to the real Unitree G1.

  </TabItem>
</Tabs>
