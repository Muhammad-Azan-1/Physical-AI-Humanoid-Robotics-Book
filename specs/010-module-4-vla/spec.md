# Module 4: Vision-Language-Action (VLA) & Humanoid Development

## 1. Overview
This module covers the final weeks of the course (Weeks 11-13), focusing on **Humanoid Robot Development** and **Vision-Language-Action (VLA)** models. It bridges the physical control of humanoids (kinematics, locomotion) with the cognitive capabilities of Large Language Models (LLMs).

### User Scenarios
- **Student** wants to understand how to make a humanoid walk (Kinematics/Dynamics).
- **Student** wants to control the robot using voice commands (OpenAI Whisper).
- **Student** wants the robot to plan tasks using an LLM ("Pick up the blue cup").

## 2. Requirements

### Functional Requirements
- **Content:**
    - **Weeks 11-12:** Humanoid Kinematics, Dynamics, Bipedal Locomotion, Manipulation.
    - **Week 13:** Conversational Robotics, GPT integration, Voice-to-Action (Whisper).
    - **Capstone:** The Autonomous Humanoid project description.
- **Structure:**
    - `physical-ai-book/docs/module4/index.md`: Overview page.
    - `physical-ai-book/docs/module4/week11-13.md`: Detailed content.
- **Enrichment:**
    - Deep dive into **Zero Moment Point (ZMP)** vs **Reinforcement Learning** for walking.
    - Code examples for **OpenAI API** integration with ROS 2.
    - Architecture diagram for the **VLA pipeline** (Voice -> Text -> Plan -> Action).

### Non-Functional Requirements
- **Consistency:** Follow "Overview + Weekly Content" structure.
- **Creativity:** Use "Humanoid-First" examples (e.g., "Unitree G1 walking").

## 3. Success Criteria
- Module 4 is accessible via sidebar.
- Content covers both physical control and cognitive AI.
- Includes code snippets for LLM integration.
